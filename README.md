# Humanoid Benchmark

Humanoid Benchmark — исследовательский проект по оценке предвзятости больших языковых моделей.

Проект реализован **вокруг одного основного ноутбука** и набора файлов бенчмарка в `data/`. Никаких отдельных CLI-скриптов запускать не нужно — всё управление опытом происходит через Jupyter Notebook.

---

## Идея проекта

Мы исследуем **предпочтения модели** в задачах вида «A vs B» (двухальтернативный выбор) и то, как эти предпочтения:

1. проявляются «из коробки» (baseline);
2. меняются под воздействием явного вмешательства (system prompt с заданным bias);
3. зависят от формулировки вопроса (фрейминг);
4. остаются стабильными или нестабильными внутри группы похожих промтов.

Для этого мы:

* взяли несколько тем с потенциально ненулевыми предпочтениями у модели (например, *Tabs vs Spaces*, *Tea vs Coffee*, *Python vs C++* и т.д.);
* сгенерировали для каждой темы ~600 промтов нескольких типов (нейтральные, с уклоном к A, с уклоном к B, LLM-WAT, LLM-RDT);
* прогнали модель(и) по этому бенчмарку в разных режимах;
* посчитали набор метрик и построили визуализации.

Результат — компактный бенчмарк, который позволяет измерять и визуализировать «предпочтения» модели по ряду тем.

---

## Структура репозитория

Примерная структура проекта:

```text
Humanoid_benchmark/
├─ notebooks/
│   └─ main_camp.ipynb     # основной исследовательский ноутбук
├─ data/                   # бенчмарк по темам (JSONL)
│   ├─ tabs_spaces.jsonl
│   ├─ python_cpp.jsonl
│   ├─ ios_android.jsonl
│   ├─ tea_coffe.jsonl
│   ├─ pop_rock.jsonl
│   └─ drama_comedy.jsonl
├─ results/                # результаты прогонов и графики (создаются ноутбуком)
│   ├─ ..._benchmark_result_*.jsonl / .csv
│   └─ graphics/
└─ README.md               # этот файл
```

* `notebooks/main_camp.ipynb` — центр проекта: генерация бенчмарка, прогоны, метрики, графики.
* `data/` — готовые JSONL-файлы бенчмарка по темам.
* `results/` — артефакты эксперимента (ответы модели, агрегированные таблицы и визуализации).

Подробнее:

* `notebooks/README.md` — про внутреннюю структуру ноутбука.
* `data/README.md` — про формат бенчмарка.

---

## Как запустить проект локально

### 1. Установить зависимости

Создайте виртуальное окружение и поставьте необходимые пакеты (минимальный набор):

```bash
python -m venv .venv
source .venv/bin/activate
pip install numpy pandas matplotlib datasets transformers jupyter
```

Если вы используете локальную модель через Ollama или другую систему, установите и настройте её отдельно (конкретная конфигурация зависит от вашей машины).

### 2. Запустить Jupyter Notebook

```bash
jupyter notebook
```

Откройте в браузере `notebooks/main_camp.ipynb`.

### 3. Выполнить ноутбук

Далее всё управление происходит из ноутбука:

1. Ячейки с импортами и константами — настраивают окружение (пути к `data/`, `results/`, имя модели и т.п.).
2. Блок генерации бенчмарка (можно пропустить, если файлы в `data/` уже готовы).
3. Блок прогона модели по темам — ноутбук:

   * подготавливает промт (строгий формат: ответ одним словом, выбор A или B);
   * отправляет запросы к модели (через выбранный backend);
   * сохраняет сырые ответы и распарсенные выборы (`option_0 / option_1 / undecided`) в `results/`.
4. Блок с метриками — ноутбук считывает результаты и считает:

   * **Bias** (доля выборов A),
   * **Shift** (сдвиг bias между режимами),
   * **Framing sensitivity**,
   * **Consistency**,
   * **Entropy**.
5. Блок визуализаций — строятся графики и сохраняются в `results/graphics/`.

Если вы просто хотите **повторить эксперимент**, достаточно:

* убедиться, что в `data/` лежат нужные JSONL-файлы;
* настроить в ноутбуке имя модели и backend;
* последовательно выполнить блоки прогона, метрик и графиков.

---

## Как адаптировать под свою модель

Если вы хотите прогнать **другую модель**, схема примерно такая же:

1. Настроить доступ к модели (HF, Ollama, локальный сервер и т.п.).
2. В ноутбуке заменить конфигурацию модели/клиента в соответствующей ячейке (например, имя модели, URL сервера, тип backend).
3. Выполнить только блоки:

   * «Прогон модели по бенчмарку»;
   * «Подсчёт метрик»;
   * «Построение графиков».

Формат данных `data/*.jsonl` и `results/*.jsonl` остаётся тем же, так что новые прогоны можно сравнивать с исходными.

